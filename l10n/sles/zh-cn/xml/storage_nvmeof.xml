<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="storage_nvmeof.xml" version="5.0" xml:id="cha-nvmeof" xml:lang="zh-cn">
 <title>NVMe over Fabric</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:bugtracker/>
        <dm:translation>no</dm:translation>
      </dm:docmanager>
      <abstract>
        <para>
    This chapter describes how to set up an NVMe over Fabric host and target.
   </para>
      </abstract>
    </info>
    <sect1 xml:id="sec-nvmeof-overview">
  <title>Overview</title>
  <para>
   <emphasis>NVM Express</emphasis> (<emphasis>NVMe</emphasis>) is an
   interface standard for accessing non-volatile storage, commonly SSD
   disks. NVMe supports much higher speeds and has a lower latency
   than SATA.
  </para>
  <para>
   <emphasis>NVMe over Fabric</emphasis> is an architecture to access NVMe
   storage over different networking fabrics, for example
   <emphasis>RDMA</emphasis> or <emphasis>NVMe over Fibre
   Channel</emphasis> (<emphasis>FC-NVMe</emphasis>). The role of
   NVMe over Fabric is similar to iSCSI. To increase the fault-tolerance,
   NVMe over Fabric has a built-in support for multipathing. The NVMe over Fabric
   multipathing is not based on the traditional DM-Multipathing.
  </para>
  <para>
   The <emphasis>NVMe host</emphasis> is the machine that connects to
   an NVMe target. The <emphasis>NVMe target</emphasis> is the
   machine that shares its NVMe block devices.
  </para>
  <para>
   NVMe is supported on SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles;sled">15 SP2</phrase></phrase>. There are Kernel
   modules available for the NVMe block storage and NVMe over Fabric target
   and host.
  </para>
  <para>
   To see if your hardware requires any special consideration, refer to
   <xref linkend="sec-nvmeof-hardware"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-nvmeof-host-configuration">
  <title>Setting Up an NVMe over Fabric Host</title>
  <para>
   To use NVMe over Fabric, a target must be available with one of the
   supported networking methods. Supported are NVMe over Fibre
   Channel and RDMA. The following sections describe how to connect a
   host to an NVMe target.
  </para>
  <sect2 xml:id="sec-nvmeof-host-configuration-cli">
   <title>Installing Command Line Client</title>
   <para>
    To use NVMe over Fabric, you need the <command>nvme</command> command line
    tool. Install it with <command>zypper</command>:
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>zypper in nvme-cli</command></screen>
   <para>
    Use <command>nvme --help</command> to list all available
    subcommands. Man pages are available for <command>nvme</command>
    subcommands. Consult them by executing <command>man
    nvme-<replaceable>SUBCOMMAND</replaceable></command>. For example,
    to view the man page for the <option>discover</option> subcommand,
    execute <command>man nvme-discover</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-target-discovery">
   <title>Discovering NVMe over Fabric Targets</title>
   <para>
    To list available NVMe subsystems on the NVMe over Fabric target, you need
    the discovery controller address and service ID.
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvme discover -t <replaceable>TRANSPORT</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying
    transport medium: <option>loop</option>, <option>rdma</option> or
    <option>fc</option>. Replace
    <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with the
    address of the discovery controller. For RDMA this should be an
    IPv4 address. Replace <replaceable>SERVICE_ID</replaceable> with
    the transport service ID. If the service is IP based, like RDMA,
    service ID specifies the port number. For Fibre Channel, the service
    ID is not required.
   </para>
   <para>
    The NVMe hosts only see the subsystems they are allowed to connect
    to.
   </para>
   <para>
    Example:
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvme discover -t rdma -a 10.0.0.1 -s 4420</command></screen>
   <para>
    For more details, see <command>man nvme-discover</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-connect-target">
   <title>Connecting to NVMe over Fabric Targets</title>
   <para>
    After you have identified the NVMe subsystem, you can connect it
    with the <command>nvme connect</command> command.
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvme connect -t <replaceable>transport</replaceable> -a <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> -s <replaceable>SERVICE_ID</replaceable> -n <replaceable>SUBSYSTEM_NQN</replaceable></command></screen>
   <para>
    Replace <replaceable>TRANSPORT</replaceable> with the underlying
    transport medium: <option>loop</option>, <option>rdma</option> or
    <option>fc</option>.
    Replace <replaceable>DISCOVERY_CONTROLLER_ADDRESS</replaceable> with
    the address of the discovery controller. For RDMA this should be an
    IPv4 address.
    Replace <replaceable>SERVICE_ID</replaceable> with the transport
    service ID. If the service is IP based, like RDMA, this specifies
    the port number.
    Replace <replaceable>SUBSYSTEM_NQN</replaceable> with the NVMe
    qualified name of the desired subsystem as found by the discovery
    command. <emphasis>NQN</emphasis> is the abbreviation for <emphasis>
    NVMe Qualified Name</emphasis>. The NQN must be unique.
   </para>
   <para>Example:</para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvme connect -t rdma -a 10.0.0.1 -s 4420 -n nqn.2014-08.com.example:nvme:nvm-subsystem-sn-d78432</command></screen>
   <para>
    Alternatively, use <command>nvme connect-all</command> to connect
    to all discovered namespaces. For advanced usage see
    <command>man nvme-connect</command> and <command>man
    nvme-connect-all</command>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-host-configuration-multipathing">
   <title>Multipathing</title>
   <para>
    NVMe native multipathing is enabled by default. To
    print the layout of the multipath devices, use the command
    <command>nvme list-subsys</command>. To disable NVMe native
    multipathing, add <option>nvme-core.multipath=N</option> as a boot
    parameter.
   </para>
   <para>
    The command <command>multipath -ll</command> has a compatibility
    mode and displays NVMe multipath devices.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-target-configuration">
  <title>Setting Up an NVMe over Fabric Target</title>
  <sect2 xml:id="sec-nvmeof-target-configuration-cli">
   <title>Installing Command Line Client</title>
   <para>
    To configure an NVMe over Fabric target, you need the <command>nvmetcli</command> command line
    tool. Install it with <command>zypper</command>:
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>zypper in nvmetcli</command></screen>
   <para>
    The current documentation for <command>nvmetcli</command> is
    available at <link xlink:href="http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/HEAD:/Documentation/nvmetcli.txt"/>.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-target-configuration-steps">
   <title>Configuration Steps</title>
   <para>
    The following procedure provides an example of how to set up an
    NVMe over Fabric target.
   </para>
   <para>
    The configuration is stored in a tree structure. Use the command
    <command>cd</command> to navigate. Use <command>ls</command> to
    list objects. You can create new objects with
    <command>create</command>.
   </para>
   <procedure>
    <step>
     <para>
      Start the <command>nvmectli</command> interactive shell:
     </para>
     <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvmetcli</command></screen>
    </step>
    <step>
     <para>
      Create a new port:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd ports</command>
<prompt>(nvmetcli)&gt; </prompt><command>create 1</command>
<prompt>(nvmetcli)&gt; </prompt><command>ls 1/</command>
o- 1
  o- referrals
  o- subsystems</screen>
    </step>
    <step>
     <para>
      Create an NVMe subsystem:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd /subsystems</command>
<prompt>(nvmetcli)&gt; </prompt><command>create nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82</command>
<prompt>(nvmetcli)&gt; </prompt><command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/</command>
<prompt>(nvmetcli)&gt; </prompt><command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces</screen>
    </step>
    <step>
     <para>
      Create a new namespace and set an NVMe device to it:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd namespaces</command>
<prompt>(nvmetcli)&gt; </prompt><command>create 1</command>
<prompt>(nvmetcli)&gt; </prompt><command>cd 1</command>
<prompt>(nvmetcli)&gt; </prompt><command>set device path=/dev/nvme0n1</command>
Parameter path is now '/dev/nvme0n1'.</screen>
    </step>
    <step>
     <para>
      Enable the previously created namespace:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd ..</command>
<prompt>(nvmetcli)&gt; </prompt><command>enable</command>
The Namespace has been enabled.</screen>
    </step>
    <step>
     <para>
      Display the created namespace:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd ..</command>
<prompt>(nvmetcli)&gt; </prompt><command>ls</command>
o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
  o- allowed_hosts
  o- namespaces
    o- 1</screen>
    </step>
    <step>
     <para>
      Allow all hosts to use the subsystem. Only do this in secure
      environments.
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>set attr allow_any_host=1</command>
Parameter allow_any_host is now '1'.</screen>
     <para>
      Alternatively, you can allow only specific hosts to connect:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82/allowed_hosts/</command>
<prompt>(nvmetcli)&gt; </prompt><command>create hostnqn</command></screen>
    </step>
    <step>
     <para>
      List all created objects:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd /</command>
<prompt>(nvmetcli)&gt; </prompt><command>ls</command>
o- /
  o- hosts
  o- ports
  | o- 1
  |   o- referrals
  |   o- subsystems
  o- subsystems
    o- nqn.2014-08.org.nvmexpress:NVMf:uuid:c36f2c23-354d-416c-95de-f2b8ec353a82
      o- allowed_hosts
      o- namespaces
        o- 1</screen>
    </step>
    <step>
     <para>
      Make the target available via RDMA:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd ports/1/</command>
<prompt>(nvmetcli)&gt; </prompt><command>set addr adrfam=ipv4 trtype=rdma traddr=10.0.0.1 trsvcid=4420</command>
Parameter trtype is now 'rdma'.
Parameter adrfam is now 'ipv4'.
Parameter trsvcid is now '4420'.
Parameter traddr is now '10.0.0.1'.</screen>
     <para>
      Alternatively, you can make it available with Fibre Channel:
     </para>
<screen><prompt>(nvmetcli)&gt; </prompt><command>cd ports/1/</command>
<prompt>(nvmetcli)&gt; </prompt><command>set addr adrfam=fc trtype=fc traddr=nn-0x1000000044001123:pn-0x2000000055001123 trsvcid=none</command></screen>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec-nvmeof-target-configuration-backup-configuration">
   <title>Back Up and Restore Target Configuration</title>
   <para>
    You can save the target configuration in a JSON file with the
    following commands:
   </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> <command>nvmetcli</command>
<prompt>(nvmetcli)&gt; </prompt><command>saveconfig nvme-target-backup.json</command></screen>
    <para>
     To restore the configuration, use:
    </para>
    <screen><prompt>(nvmetcli)&gt; </prompt><command>restore nvme-target-backup.json</command></screen>
    <para>
     You can also wipe the current configuration:
    </para>
    <screen><prompt>(nvmetcli)&gt; </prompt><command>clear</command></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-hardware">
  <title>Special Hardware Configuration</title>
  <sect2 xml:id="sec-nvmeof-hardware-overview">
   <title>Overview</title>
   <para>
    Some hardware needs special configuration to work correctly. Skim
    the titles of the following sections to see if you are using any of
    the mentioned devices or vendors.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-hardware-broadcom">
   <title>Broadcom</title>
   <para>
    If you are using the <emphasis>Broadcom Emulex LightPulse Fibre
    Channel SCSI</emphasis> driver, add a Kernel configuration
    parameter on the target and host for the <literal>lpfc</literal>
    module:
   </para>
   <screen><prompt>tux &gt; </prompt><command>sudo</command> <command>echo "options lpfc lpfc_enable_fc4_type=3" &gt; /etc/modprobe.d/lpfc.conf</command></screen>
   <para>
    Make sure that the Broadcom adapter firmware has at least version
    11.4.204.33. Also make sure that you have the current versions of
    <package>nvme-cli</package>, <package>nvmetcli</package> and the
    Kernel installed.
   </para>
   <para>
    To enable a Fibre Channel port as an NVMe target, an additional
    module parameter needs to be configured:
    <option>lpfc_enable_nvmet=<replaceable>
    COMMA_SEPARATED_WWPNS</replaceable></option>. Enter the WWPN with a
    leading <literal>0x</literal>, for example
    <option>lpfc_enable_nvmet=0x2000000055001122,0x2000000055003344</option>.
    Only listed WWPNs will be configured for target mode. A Fibre
    Channel port can either be configured as target
    <emphasis>or</emphasis> as initiator.
   </para>
  </sect2>
  <sect2 xml:id="sec-nvmeof-hardware-marvell">
   <title>Marvell</title>
   <para>
    FC-NVMe is supported on QLE269x and QLE27xx adapters. FC-NVMe support
    is enabled by default in the Marvell® QLogic® QLA2xxx Fibre Channel driver.
   </para>
   <para>
    To confirm NVMe is enabled, run the following command:
   </para>
   <screen><prompt>tux &gt; </prompt>cat /sys/module/qla2xxx/parameters/ql2xnvmeenable</screen>
   <para>
    A resulting <literal>1</literal> means NVMe is enabled, a
    <literal>0</literal> indicates it is disabled.
   </para>

   <para>
    Next, ensure that the Marvell adapter firmware is at least version
    8.08.204 by checking the output of the following command:
   </para>
   <screen><prompt>tux &gt; </prompt>cat /sys/class/scsi_host/host0/fw_version</screen>

   <para>
    Last, ensure that the latest versions available for <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise Server</phrase></phrase> of
    <package>nvme-cli</package>, <package>QConvergeConsoleCLI</package>, and
    the Kernel are installed. You may, for example, run
   </para>
   <screen><prompt role="root">root # </prompt>zypper lu &amp;&amp; zypper pchk</screen>
   <para>
    to check for updates and patches.
   </para>

   <para>
    For more details on installation, please refer to the FC-NVMe sections in
    the following Marvell user guides:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32769&amp;docid=96728&amp;ProductCategory=39&amp;Product=1259&amp;Os=126"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="http://driverdownloads.qlogic.com/QLogicDriverDownloads_UI/ShowEula.aspx?resourceid=32761&amp;docid=96726&amp;ProductCategory=39&amp;Product=1261&amp;Os=126"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-nvmeof-more-information">
  <title>More Information</title>
  <para>
   For more details about the abilities of the <command>nvme</command> command,
   refer to <command>nvme nvme-help</command>.
  </para>
  <para>
   The following links provide a basic introduction to NVMe and
   NVMe over Fabric:
  </para>
  <itemizedlist>
   <listitem>
    <para><link xlink:href="http://nvmexpress.org/"/></para>
   </listitem>
   <listitem>
    <para><link xlink:href="http://www.nvmexpress.org/wp-content/uploads/NVMe_Over_Fabrics.pdf"/></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://storpool.com/blog/demystifying-what-is-nvmeof"/></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://medium.com/@metebalci/a-quick-tour-of-nvm-express-nvme-3da2246ce4ef"/></para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
